# üéÇ ELT/ETL
**Web scraping** √© o processo de extra√ß√£o automatizada de informa√ß√µes de p√°ginas da web, simulando o comportamento de um usu√°rio humano, mas de forma program√°tica. Em vez de copiar dados manualmente de um site, um script ou programa acessa o conte√∫do HTML da p√°gina, interpreta sua estrutura e coleta as informa√ß√µes desejadas ‚Äî como textos, links, imagens, pre√ßos, not√≠cias ou qualquer dado dispon√≠vel publicamente.

Em resumo, o web scraping √© uma forma de coletar dados estruturados a partir de fontes n√£o estruturadas na web ‚Äî transformando p√°ginas HTML em datasets √∫teis para an√°lise, aprendizado de m√°quina, automa√ß√£o e relat√≥rios, desde que feito com responsabilidade e dentro dos limites √©ticos e legais.

![FB_IMG_1735839034899](https://github.com/user-attachments/assets/9f2416bb-e69c-4115-a296-e29d8772f503)
![FB_IMG_1735839040474](https://github.com/user-attachments/assets/4264a5e2-45f3-45a9-8256-71477561a9cb)
![FB_IMG_1735839051637](https://github.com/user-attachments/assets/da293070-2dd5-4379-8a54-95c558ad9cb5)
![FB_IMG_1735839060017](https://github.com/user-attachments/assets/becb6f34-6db3-4a10-b1ea-a4469759dab1)

Tecnicamente, o web scraping funciona em tr√™s etapas principais:

1. Primeiro, o programa **envia uma requisi√ß√£o HTTP** para o servidor do site, assim como um navegador faria quando voc√™ acessa uma p√°gina.
2. Em seguida, ele **recebe o HTML bruto** como resposta.
3. Por fim, esse HTML √© **analisado e interpretado**, geralmente com bibliotecas que entendem a estrutura do documento (como `BeautifulSoup`, `lxml` ou `Scrapy` no Python), para extrair os elementos espec√≠ficos que voc√™ quer, identificados por tags, classes, IDs ou atributos.

Esse processo √© amplamente usado em an√°lise de dados, ci√™ncia de dados, monitoramento de pre√ßos, coleta de not√≠cias, SEO, e at√© intelig√™ncia de mercado, quando APIs n√£o est√£o dispon√≠veis. 

Por exemplo, se uma loja online n√£o oferece uma API p√∫blica, voc√™ pode criar um scraper que visita as p√°ginas de produtos e extrai automaticamente nomes, valores e descri√ß√µes.

No entanto, h√° uma quest√£o √©tica e legal importante: nem todos os sites permitem scraping. Muitos t√™m restri√ß√µes no arquivo `robots.txt`, que define o que pode ou n√£o ser acessado por scripts automatizados. Al√©m disso, scraping em excesso pode sobrecarregar servidores e ser considerado uso indevido. Por isso, boas pr√°ticas incluem respeitar limites de requisi√ß√£o, identificar o agente de usu√°rio (user-agent) e nunca raspar dados pessoais ou protegidos por login.

- https://github.com/microsoft/markitdown
- https://github.com/browser-use/browser-use
- https://python.plainenglish.io/full-apache-airflow-tutorial-interview-notes-zero-to-ready-31b5e3ae5b56
